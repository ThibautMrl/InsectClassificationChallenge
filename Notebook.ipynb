{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kaggle Challenge"
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The objective of this challenge was to classify 102 different insects present on approximately 23000 test images with about 53000 training data at our disposal. I decided to start with transfer learning and I relied on these different keras documentations for that:\n",
    "\n",
    "https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/  \n",
    "https://keras.io/api/applications/  \n",
    "https://keras.io/api/applications/#usage-examples-for-image-classification-models\n",
    "\n",
    "Knowing that the number of data was limited and the training time too, I wanted a pre-trained model with a reasonable number of parameters and a good accuracy. I wanted to turn first to efficientNet algorithms having very good results with few parameters but during the training of the model there was a recurrent error on the efficientNet that I did not have on the other models so I could not use them (the training work but in a very inefficient way). After analysis and testing on other models I decided to use the InceptionV3.\n",
    "During this challenge,the main problem was to manage the class imbalance by using the f1-score metrics and taking into account the weight in classes without this the f1-score we were evaluated on was very bad.\n",
    "And the difficulty that restricted me the most was not so much the 30 hours of GPU available but rather the 12 hours maximum of execution of a kaggle notebook. Indeed, I had the surprise that my training was not finished and that nothing could be saved and that I lost 12 hours. I am now much more aware of the time needed for each task, I also learned to save the weights of my models so that I don't lose them and have to start the training again each time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TensorFlow\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:12:45.272370Z",
     "iopub.execute_input": "2023-04-21T19:12:45.272811Z",
     "iopub.status.idle": "2023-04-21T19:12:55.825030Z",
     "shell.execute_reply.started": "2023-04-21T19:12:45.272762Z",
     "shell.execute_reply": "2023-04-21T19:12:55.823947Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the importation of the data, being given that they were not all of the same size and that the labels were stored in a separate csv file I left on an imageDataGenerator. This one allows of course not to have to load all the images at once and saturate the ram, to preprocess the images as one wishes it (in particular the adaptation of the size images without cropping them not to lose information) and allows to make data increase by the same occasion thus that corresponded completely to what I wished. For the preprocessing of the data a function dedicated to the Inception model exists in Keras, for the data augmentation I used rotation/translation/version of the images and enlargement/reduction of them.\n",
    "I decide to use a small batch size number because of the complexity of the data set, so i took 32."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#Setting important parameters\n",
    "image_size = 299 #Size required for the InceptionV3 model\n",
    "batch_size = 32"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:13:01.141999Z",
     "iopub.execute_input": "2023-04-21T19:13:01.142834Z",
     "iopub.status.idle": "2023-04-21T19:13:01.411678Z",
     "shell.execute_reply.started": "2023-04-21T19:13:01.142790Z",
     "shell.execute_reply": "2023-04-21T19:13:01.410353Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load labels from CSV file\n",
    "df = pd.read_csv('/kaggle/input/polytechnice2023-deep-learning-competition/train.csv')\n",
    "df['y'] = df['y'].astype(str)\n",
    "\n",
    "# Define data transformations\n",
    "# Preprocessing + Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input,#Preprocessing images for InceptionV3\n",
    "    rotation_range=20,# Rotate images\n",
    "    width_shift_range=0.2,# Shifting the image to the sides\n",
    "    height_shift_range=0.2,# Same upwards\n",
    "    zoom_range=0.2,# Enlargement/Reduction of the image\n",
    "    horizontal_flip=True,# Horizontal image inversion\n",
    "    vertical_flip=True,# Same Vertical\n",
    "    validation_split=0.2 # Separation of data in validation and train\n",
    ")\n",
    "\n",
    "# Pre-processing of training data\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,# The dataframe where the labels are stored\n",
    "    #The directory where the images are stored\n",
    "    directory='/kaggle/input/polytechnice2023-deep-learning-competition/train_images',\n",
    "    x_col='ID',# Where the image names are stored\n",
    "    y_col= 'y',# Where the labels are stored\n",
    "    target_size=(image_size,image_size),#Adapting the image to the required size\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Using training data\n",
    ")\n",
    "\n",
    "# Pre-processing of validation data\n",
    "valid_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory='/kaggle/input/polytechnice2023-deep-learning-competition/train_images',\n",
    "    x_col='ID',\n",
    "    y_col='y',\n",
    "    target_size=(image_size,image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use validation data\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T12:15:20.294578Z",
     "iopub.execute_input": "2023-04-17T12:15:20.295611Z",
     "iopub.status.idle": "2023-04-17T12:17:04.477986Z",
     "shell.execute_reply.started": "2023-04-17T12:15:20.295570Z",
     "shell.execute_reply": "2023-04-17T12:17:04.476782Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_weights = compute_class_weight(class_weight = 'balanced',\n",
    "                                     classes = np.unique(train_generator.classes),\n",
    "                                     y = train_generator.classes)\n",
    "class_weight_dict = dict(zip(np.unique(train_generator.classes), class_weights))\n",
    "print(class_weight_dict)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-16T22:29:20.618745Z",
     "iopub.execute_input": "2023-04-16T22:29:20.619482Z",
     "iopub.status.idle": "2023-04-16T22:29:20.721399Z",
     "shell.execute_reply.started": "2023-04-16T22:29:20.619437Z",
     "shell.execute_reply": "2023-04-16T22:29:20.720196Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the dictionary to take into account the weight of the classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Images and labels for the next batch\n",
    "images, labels = next(train_generator)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(images[0])\n",
    "plt.title('Label: {}'.format(labels[0]))\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-16T22:29:20.724752Z",
     "iopub.execute_input": "2023-04-16T22:29:20.725078Z",
     "iopub.status.idle": "2023-04-16T22:29:22.129423Z",
     "shell.execute_reply.started": "2023-04-16T22:29:20.725050Z",
     "shell.execute_reply": "2023-04-16T22:29:22.128353Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The choice of the number of epochs was imposed on me by the 12 hours of the kaggle so I decided to do 15 for the training but in the end I only used 10 because I noticed that we quickly reach the maximum precision by training the last layers, certainly because the model has not been trained on insect recognition and our dataset is complicated, that's why I finally put more epoch for the fine tuning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of classes and epochs\n",
    "epochs = 10\n",
    "classes = 102"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:13:47.540404Z",
     "iopub.execute_input": "2023-04-21T19:13:47.541214Z",
     "iopub.status.idle": "2023-04-21T19:13:47.546008Z",
     "shell.execute_reply.started": "2023-04-21T19:13:47.541168Z",
     "shell.execute_reply": "2023-04-21T19:13:47.544714Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of the model with a GlobalAveragePooling2D layer to reduce x to a 1D vector, BatchNormalization and dropout to facilitate learning (as for the dropout rate I didn't really have the time to try several so I took the one from the keras doc) and finally 2 Dense layers with one fully connected and one prediction layer for the 102 classes\n",
    "First I freeze the pre-trainer model so that the prediction layers can be trained in priority."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D,Dropout,BatchNormalization\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    # Loading InceptionV3 without the last layers of classifications\n",
    "    # weights='imagenet' to load the weights\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    # Adding the classification layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    top_dropout_rate = 0.2\n",
    "    x = Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    x = Dense(1280, activation='relu')(x)\n",
    "    predictions = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    # New model\n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze of pre-trained model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:13:49.981066Z",
     "iopub.execute_input": "2023-04-21T19:13:49.981523Z",
     "iopub.status.idle": "2023-04-21T19:13:52.944935Z",
     "shell.execute_reply.started": "2023-04-21T19:13:49.981472Z",
     "shell.execute_reply": "2023-04-21T19:13:52.943724Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the optimizer I chose rmsprop as indicated in the keras doc and it worked very well (I tried with Adam but the difference was not very noticeable in the time I had) with a loss categorical_crossentropy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow_addons.metrics import F1Score\n",
    "f1score = F1Score(num_classes=classes, average='macro')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-16T22:29:25.733668Z",
     "iopub.execute_input": "2023-04-16T22:29:25.734068Z",
     "iopub.status.idle": "2023-04-16T22:29:26.010854Z",
     "shell.execute_reply.started": "2023-04-16T22:29:25.734028Z",
     "shell.execute_reply": "2023-04-16T22:29:26.009775Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compilation and training of the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[f1score])\n",
    "history = model.fit(train_generator, epochs=epochs, \n",
    "                    validation_data=valid_generator,class_weight=class_weight_dict)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-16T22:29:57.717480Z",
     "iopub.execute_input": "2023-04-16T22:29:57.718532Z",
     "iopub.status.idle": "2023-04-17T02:36:39.688859Z",
     "shell.execute_reply.started": "2023-04-16T22:29:57.718493Z",
     "shell.execute_reply": "2023-04-17T02:36:39.687829Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#To save and download the weights of my model\n",
    "model.save_weights('my_model_weights.h5')\n",
    "!cd /kaggle/working\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'my_model_weights.h5')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T02:36:39.691916Z",
     "iopub.execute_input": "2023-04-17T02:36:39.692708Z",
     "iopub.status.idle": "2023-04-17T02:36:41.251051Z",
     "shell.execute_reply.started": "2023-04-17T02:36:39.692666Z",
     "shell.execute_reply": "2023-04-17T02:36:41.249802Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"f1_score\"])\n",
    "    plt.plot(hist.history[\"val_f1_score\"])\n",
    "    plt.title(\"model f1_score\")\n",
    "    plt.ylabel(\"f1_score\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(hist)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we see that the model trains well with a validation curve that increases as the training curve increases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine Tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the fine tuning I did not unfreeze the whole model as indicated in the keras doc, it seemed coherent to me to have less parameters to train given our quantity of images at disposal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Defreeze certaines couche du model préentrainé\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T02:36:41.253395Z",
     "iopub.execute_input": "2023-04-17T02:36:41.253904Z",
     "iopub.status.idle": "2023-04-17T02:36:41.271976Z",
     "shell.execute_reply.started": "2023-04-17T02:36:41.253857Z",
     "shell.execute_reply": "2023-04-17T02:36:41.270584Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "I decided to use early stopping to prevent overfitting but this was not necessary as I could not train my model long enough for this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "ourCallback = EarlyStopping(monitor='f1_score', min_delta=0.0001, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T02:36:41.275062Z",
     "iopub.execute_input": "2023-04-17T02:36:41.275778Z",
     "iopub.status.idle": "2023-04-17T02:36:41.283338Z",
     "shell.execute_reply.started": "2023-04-17T02:36:41.275722Z",
     "shell.execute_reply": "2023-04-17T02:36:41.282313Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "I try many optimizer for fine tuning like SGD(lr=0.0001, momentum=0.9) in the keras documentation but my accuracy/f1 score fall drastically. I try RMSprop just with a really low step and that was great i have a good fine tuning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Compilation and training\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "epochs = 15\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.00001), loss='categorical_crossentropy', metrics=[f1score])\n",
    "history = model.fit(train_generator, epochs=epochs, \n",
    "                    validation_data=valid_generator,callbacks=[ourCallback],verbose=1,class_weight=class_weight_dict)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T02:36:41.285013Z",
     "iopub.execute_input": "2023-04-17T02:36:41.285377Z",
     "iopub.status.idle": "2023-04-17T08:42:21.817072Z",
     "shell.execute_reply.started": "2023-04-17T02:36:41.285342Z",
     "shell.execute_reply": "2023-04-17T08:42:21.815951Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#To save and download the weights of my model\n",
    "model.save_weights('my_model_weights.h5')\n",
    "!cd /kaggle/working\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'my_model_weights.h5')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T08:42:22.353470Z",
     "iopub.execute_input": "2023-04-17T08:42:22.354396Z",
     "iopub.status.idle": "2023-04-17T08:42:24.256706Z",
     "shell.execute_reply.started": "2023-04-17T08:42:22.354361Z",
     "shell.execute_reply": "2023-04-17T08:42:24.255369Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "I had originally put another train phase by freezing the pre-trainer model I thought it could have been useful but no it was a bad idea I lost accuracy so I decided to remove it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the test data I had to batch load it by hand because with an image generator and flow_from_directory it saturated my RAM.\n",
    "Then I simply predicted and stored the data in a dataframe that I converted to csv."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#Load weights of my model\n",
    "model.load_weights('/kaggle/input/weigths/my_model_weights (4).h5')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:14:06.462191Z",
     "iopub.execute_input": "2023-04-21T19:14:06.462722Z",
     "iopub.status.idle": "2023-04-21T19:14:08.018667Z",
     "shell.execute_reply.started": "2023-04-21T19:14:06.462678Z",
     "shell.execute_reply": "2023-04-21T19:14:08.017048Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Loading test data\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# path of the directory where the images are\n",
    "image_dir = '/kaggle/input/polytechnice2023-deep-learning-competition/test_images'\n",
    "# All paths of all images\n",
    "test_images = [\"/kaggle/input/polytechnice2023-deep-learning-competition/train_images/00006.jpg\",\n",
    "               \"/kaggle/input/polytechnice2023-deep-learning-competition/train_images/00007.jpg\",\n",
    "              \"/kaggle/input/polytechnice2023-deep-learning-competition/train_images/00008.jpg\",\n",
    "              \"/kaggle/input/polytechnice2023-deep-learning-competition/train_images/00009.jpg\",\n",
    "              \"/kaggle/input/polytechnice2023-deep-learning-competition/train_images/00010.jpg\"]\n",
    "#test_images = glob.glob(image_dir + '/*.jpg')\n",
    "\n",
    "def test_data_generator(batch_size):\n",
    "    i = 0\n",
    "    while i < len(test_images):\n",
    "        batch_images = test_images[i:i+batch_size] #batch images\n",
    "        #Array empty for batch images a preprocess\n",
    "        batch_data = np.zeros((len(batch_images), image_size, image_size, 3))\n",
    "        #Batch image path list\n",
    "        batch_filenames = []\n",
    "        for j, img_path in enumerate(batch_images):\n",
    "            img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "            x = image.img_to_array(img)\n",
    "            x = preprocess_input(x)#Preprocessing of the image\n",
    "            batch_data[j] = x\n",
    "            batch_filenames.append(os.path.basename(img_path))\n",
    "        i += batch_size\n",
    "        yield batch_data,batch_filenames\n",
    "\n",
    "batch_size = 256\n",
    "test_data = test_data_generator(batch_size)\n",
    "\n",
    "# Label prediction\n",
    "predictions_list = []\n",
    "filename_list = []\n",
    "for batch_data,batch_filenames in test_data:\n",
    "    predictions = model.predict(batch_data)\n",
    "    predictions_cat = np.argmax(predictions, axis=1)#Converting labels to int\n",
    "    print(predictions_cat)\n",
    "    predictions_list.extend(predictions_cat)\n",
    "    filename_list.extend(batch_filenames)\n",
    "    print(filename_list)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:24:46.484946Z",
     "iopub.execute_input": "2023-04-21T19:24:46.485338Z",
     "iopub.status.idle": "2023-04-21T19:24:47.175569Z",
     "shell.execute_reply.started": "2023-04-21T19:24:46.485303Z",
     "shell.execute_reply": "2023-04-21T19:24:47.174556Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Creation of a dataFrame with predictions\n",
    "results = pd.DataFrame({\n",
    "    \"ID\": filename_list,\n",
    "    \"Prediction\": predictions_list,\n",
    "})\n",
    "# Convert it to csv\n",
    "results.to_csv(\"test_results.csv\", index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:15:48.981223Z",
     "iopub.execute_input": "2023-04-21T19:15:48.981646Z",
     "iopub.status.idle": "2023-04-21T19:15:48.998262Z",
     "shell.execute_reply.started": "2023-04-21T19:15:48.981607Z",
     "shell.execute_reply": "2023-04-21T19:15:48.997052Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# To download it\n",
    "!cd /kaggle/working\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'test_results.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-21T19:15:52.101459Z",
     "iopub.execute_input": "2023-04-21T19:15:52.102006Z",
     "iopub.status.idle": "2023-04-21T19:15:53.229791Z",
     "shell.execute_reply.started": "2023-04-21T19:15:52.101960Z",
     "shell.execute_reply": "2023-04-21T19:15:53.228667Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "After Monday's session I tried with images from the train dataset and the predictions are not good so the error is not in the conversion of my results to csv. I spent a lot of time on it but I can't see where my error is which is a bit of a shame because my training and validation results are relatively good."
   ],
   "metadata": {}
  }
 ]
}
